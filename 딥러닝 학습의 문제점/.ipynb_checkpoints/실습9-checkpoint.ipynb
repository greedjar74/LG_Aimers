{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0cc1ba7",
   "metadata": {},
   "source": [
    "# 과적합(Overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e021ac7",
   "metadata": {},
   "source": [
    "실습\n",
    "과적합 될 모델과 비교하기 위해 아래와 같이 기본 모델을 생성합니다.\n",
    "basic_model = tf.keras.Sequential([ tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)), tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "tf.keras.layers.Dense(1, activation= 'sigmoid')])\n",
    "Copy\n",
    "기본 모델을 응용해 과적합 될 모델을 생성합니다.\n",
    "\n",
    "모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "두 모델에 대해 손실 함수(loss function), 최적화(optimize) 알고리즘, 평가 방법(metrics)은 다음과 같이 설정합니다.\n",
    "\n",
    "손실 함수(loss) : ‘binary_crossentropy’\n",
    "최적화 알고리즘(optimizer) : ‘adam’\n",
    "평가 방법(metrics): [‘accuracy’, ‘binary_crossentropy’]\n",
    "Tips!\n",
    "과적합 된 모델 특성상 학습 시간이 매우 오래 걸릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380abe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING) \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "    \n",
    "'''\n",
    "1. 과적합 될 모델과 비교하기 위해 기본 모델을 \n",
    "   마크다운 설명과 동일하게 생성합니다.\n",
    "'''\n",
    "\n",
    "def Basic(word_num):\n",
    "    \n",
    "    basic_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(256, activation = 'relu', input_shape=(word_num,)), \n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation= 'sigmoid')])\n",
    "    \n",
    "    return basic_model\n",
    "\n",
    "'''\n",
    "2. 기본 모델의 레이어 수와 노드 수를 자유롭게 늘려서\n",
    "   과적합 될 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "def Overfitting(word_num):\n",
    "    \n",
    "    overfit_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1024, activation = 'relu', input_shape=(word_num,)), \n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(32, activation = 'relu'),\n",
    "        tf.keras.layers.Dense(1, activation= 'sigmoid')])\n",
    "    \n",
    "    return overfit_model\n",
    "\n",
    "'''\n",
    "3. 두 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. basic_model와 overfit_model 함수를 이용해 \n",
    "           두 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, \n",
    "           평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 학습시킵니다. \n",
    "           검증용 데이터도 설정해주세요.\n",
    "           \n",
    "           기본 모델은 'epochs'를 20, \n",
    "           과적합 모델은 'epochs'를 300이상으로 설정합니다.\n",
    "           'batch_size'는 두 모델 모두 500으로 설정합니다.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 \n",
    "           binary crossentropy 값을 출력합니다. \n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    basic_model = Basic()    # 기본 모델입니다.\n",
    "    overfit_model = Overfitting()  # 과적합시킬 모델입니다.\n",
    "    \n",
    "    basic_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy'])\n",
    "    overfit_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy'])\n",
    "    \n",
    "    basic_model.summary()\n",
    "    overfit_model.summary()\n",
    "    \n",
    "    basic_history = basic_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_data), verbose=0)\n",
    "    print('\\n')\n",
    "    overfit_history = overfit_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_data), verbose=0)\n",
    "    \n",
    "    scores_basic = basic_model.evaluate(test_data, test_labels,verbose=0)\n",
    "    scores_overfit = overfit_model.evaluate(test_data, test_labels, verbose=0)\n",
    "    \n",
    "    print('\\nscores_basic: ', scores_basic[-1])\n",
    "    print('scores_overfit: ', scores_overfit[-1])\n",
    "    \n",
    "    Visualize([('Basic', basic_history),('Overfitting', overfit_history)])\n",
    "    \n",
    "    return basic_history, overfit_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb3dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING) \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "    \n",
    "'''\n",
    "1. 과적합 될 모델과 비교하기 위해 기본 모델을 \n",
    "   마크다운 설명과 동일하게 생성합니다.\n",
    "'''\n",
    "\n",
    "def Basic(word_num):\n",
    "    \n",
    "    basic_model = tf.keras.Sequential([None])\n",
    "    \n",
    "    return basic_model\n",
    "\n",
    "'''\n",
    "2. 기본 모델의 레이어 수와 노드 수를 자유롭게 늘려서\n",
    "   과적합 될 모델을 생성합니다.\n",
    "'''\n",
    "\n",
    "def Overfitting(word_num):\n",
    "    \n",
    "    overfit_model = tf.keras.Sequential([None])\n",
    "    \n",
    "    return overfit_model\n",
    "\n",
    "'''\n",
    "3. 두 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. basic_model와 overfit_model 함수를 이용해 \n",
    "           두 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, \n",
    "           평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 학습시킵니다. \n",
    "           검증용 데이터도 설정해주세요.\n",
    "           \n",
    "           기본 모델은 'epochs'를 20, \n",
    "           과적합 모델은 'epochs'를 300이상으로 설정합니다.\n",
    "           'batch_size'는 두 모델 모두 500으로 설정합니다.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 \n",
    "           binary crossentropy 값을 출력합니다. \n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    basic_model = None    # 기본 모델입니다.\n",
    "    overfit_model = None  # 과적합시킬 모델입니다.\n",
    "    \n",
    "    basic_model.compile(None)\n",
    "    overfit_model.compile(None)\n",
    "    \n",
    "    None\n",
    "    None\n",
    "    \n",
    "    basic_history = None\n",
    "    print('\\n')\n",
    "    overfit_history = None\n",
    "    \n",
    "    scores_basic = None\n",
    "    scores_overfit = None\n",
    "    \n",
    "    print('\\nscores_basic: ', scores_basic[-1])\n",
    "    print('scores_overfit: ', scores_overfit[-1])\n",
    "    \n",
    "    Visualize([('Basic', basic_history),('Overfitting', overfit_history)])\n",
    "    \n",
    "    return basic_history, overfit_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
