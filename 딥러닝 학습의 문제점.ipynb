{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade00d49",
   "metadata": {},
   "source": [
    "# 실습1\n",
    "- tf.keras.layers.Dense(32, input_shape=(word_num,), activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "- tf.keras.layers.Dense(32, input_shape=(word_num,), activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "- GD_model(word_num)\n",
    "- SGD_model(word_num)\n",
    "- loss = 'binary_crossentropy', optimizer='sgd', metrics = ['accuracy','binary_crossentropy']\n",
    "- loss = 'binary_crossentropy', optimizer='sgd', metrics = ['accuracy','binary_crossentropy']\n",
    "- gd_model.summary()\n",
    "- sgd_model.summary()\n",
    "- gd_model.fit(train_data, train_labels, epochs=20, batch_size=data_num, validation_data = (test_data, test_labels), verbose=0)\n",
    "- sgd_model.fit(train_data, train_labels, epochs=20, batch_size=data_num, validation_data = (test_data, test_labels), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aedeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0 \n",
    "        \n",
    "    return results\n",
    "\n",
    "'''\n",
    "1. GD를 적용할 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "def GD_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([None])\n",
    "    \n",
    "    return model\n",
    "    \n",
    "'''\n",
    "2. SGD를 적용할 모델을 GD를 적용할 모델과 똑같이 생성합니다.\n",
    "'''\n",
    "\n",
    "def SGD_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([None])\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. GD 함수와 SGD 함수를 이용해 \n",
    "           두 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, \n",
    "           평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 각각 학습시킵니다. \n",
    "           검증용 데이터도 설정해주세요.\n",
    "           'epochs'는 20으로 설정합니다.\n",
    "   \n",
    "           GD를 적용할 경우 학습 시 \n",
    "           전체 데이터 셋(full-batch)을\n",
    "           사용하므로 'batch_size'를 \n",
    "           전체 데이터 개수로 설정합니다. \n",
    "           \n",
    "           SGD를 적용할 경우 학습 시 \n",
    "           미니 배치(mini-batch)를 사용하므로\n",
    "           'batch_size'를 전체 데이터 개수보다 \n",
    "           작은 수로 설정합니다. \n",
    "           \n",
    "           여기선 500으로 설정하겠습니다.\n",
    "   \n",
    "   Step05. 학습된 두 모델을 테스트하고 \n",
    "           binary crossentropy 값을 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    gd_model = None   # GD를 사용할 모델입니다.\n",
    "    sgd_model = None  # SGD를 사용할 모델입니다.\n",
    "    \n",
    "    gd_model.compile(None)\n",
    "    sgd_model.compile(None)\n",
    "    \n",
    "    None\n",
    "    None\n",
    "    \n",
    "    gd_history = None\n",
    "    print('\\n')\n",
    "    sgd_history = None\n",
    "    \n",
    "    scores_gd = gd_history.history['val_binary_crossentropy'][-1]\n",
    "    scores_sgd = sgd_history.history['val_binary_crossentropy'][-1]\n",
    "    \n",
    "    print('\\nscores_gd: ', scores_gd)\n",
    "    print('scores_sgd: ', scores_sgd)\n",
    "    \n",
    "    Visulaize([('GD', gd_history),('SGD', sgd_history)])\n",
    "    \n",
    "    return gd_history, sgd_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08026d4",
   "metadata": {},
   "source": [
    "# 실습2\n",
    "- tf.keras.layers.Dense(32, input_shape=(word_num,), activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "- Momentum_model(word_num)\n",
    "- Momentum_model(word_num)\n",
    "- tf.keras.optimizers.SGD(0.01, momentum=0)\n",
    "- loss='binary_crossentropy',optimizer=sgd_opt,metrics=['accuracy', 'binary_crossentropy']\n",
    "- tf.keras.optimizers.SGD(0.01, momentum=0.9)\n",
    "- loss='binary_crossentropy',optimizer=sgd_opt,metrics=['accuracy', 'binary_crossentropy']\n",
    "- sgd_model.summary()\n",
    "- msgd_model.summary()\n",
    "- sgd_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels), verbose=0)\n",
    "- msgd_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels), verbose=0)\n",
    "- sgd_model.evaluate(test_data, test_labels)\n",
    "- msgd_model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def8001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0 \n",
    "    \n",
    "    return results\n",
    "    \n",
    "'''\n",
    "1. 모멘텀(momentum)을 적용/비적용 할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "    \n",
    "def Momentum_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([None])\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "2. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. Momentum_model 함수를 이용해 \n",
    "           두 모델을 불러옵니다. 모두 동일한 모델입니다.\n",
    "   \n",
    "   Step02. 두 모델의 손실 함수, 최적화 알고리즘, \n",
    "           평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 두 모델을 각각 학습시킵니다. \n",
    "           검증용 데이터도 설정해주세요.\n",
    "           두 모델 모두 'epochs'는 20, 'batch_size'는\n",
    "           500으로 설정합니다.\n",
    "   \n",
    "   Step05. 학습된 두 모델을 테스트하고 \n",
    "           binary crossentropy 값을 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    sgd_model = None   # 모멘텀을 사용하지 않을 모델입니다.\n",
    "    msgd_model = None  # 모멘텀을 사용할 모델입니다.\n",
    "    \n",
    "    sgd_opt = None\n",
    "    sgd_model.compile(None)\n",
    "    \n",
    "    msgd_opt = None\n",
    "    msgd_model.compile(None)\n",
    "    \n",
    "    None\n",
    "    None\n",
    "    \n",
    "    sgd_history = None\n",
    "    print('\\n')\n",
    "    msgd_history = None\n",
    "    \n",
    "    scores_sgd = None\n",
    "    scores_msgd = None\n",
    "    \n",
    "    print('\\nscores_sgd: ', scores_sgd[-1])\n",
    "    print('scores_msgd: ', scores_msgd[-1])\n",
    "    \n",
    "    Visulaize([('SGD', sgd_history),('mSGD', msgd_history)])\n",
    "    \n",
    "    return sgd_history, msgd_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ec887",
   "metadata": {},
   "source": [
    "# 실습3\n",
    "- tf.keras.layers.Dense(32, input_shape=(word_num, ), activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "- OPT_model(word_num)\n",
    "- OPT_model(word_num)\n",
    "- OPT_model(word_num)\n",
    "- tf.keras.optimizers.Adagrad(lr=0.01, epsilon=0.0001, decay=0.4)\n",
    "- loss='binary_crossentropy', optimizer=adagrad_opt, metrics=['accuracy', 'binary_crossentropy']\n",
    "- tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "- loss='binary_crossentropy', optimizer=adagrad_opt, metrics=['accuracy', 'binary_crossentropy']\n",
    "- tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
    "- loss='binary_crossentropy', optimizer=adam_opt, metrics=['accuracy', 'binary_crossentropy']\n",
    "- adagrad_model.summary()\n",
    "- rmsprop_model.summary()\n",
    "- adam_model.summary()\n",
    "- adagrad_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels), verbose=0)\n",
    "- rmsprop_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels), verbose=0)\n",
    "- adam_model.fit(train_data, train_labels, epochs=20, batch_size=500, validation_data=(test_data, test_labels), verbose=0)\n",
    "- adagrad_model.evaluate(test_data, test_labels, verbose=0)\n",
    "- rmsprop_model.evaluate(test_data, test_labels, verbose=0)\n",
    "- adam_model.evaluate(test_data, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# 데이터를 전처리하는 함수\n",
    "\n",
    "def sequences_shaping(sequences, dimension):\n",
    "    \n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, word_indices in enumerate(sequences):\n",
    "        results[i, word_indices] = 1.0 \n",
    "    \n",
    "    return results\n",
    "\n",
    "'''\n",
    "1. Adagrad, RMSprop, Adam 최적화 알고리즘을 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "def OPT_model(word_num):\n",
    "    \n",
    "    model = tf.keras.Sequential([None])\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "2. 세 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. OPT_model 함수를 이용해 세 모델을 불러옵니다. \n",
    "           모두 동일한 모델입니다.\n",
    "   \n",
    "   Step02. 세 모델의 손실 함수, 최적화 방법, \n",
    "           평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "   \n",
    "   Step04. 세 모델을 각각 학습시킵니다. \n",
    "           세 모델 모두 'epochs'는 20, 'batch_size'는\n",
    "           500으로 설정합니다.\n",
    "   \n",
    "   Step05. 세 모델을 테스트하고 \n",
    "           binary crossentropy 점수를 출력합니다. \n",
    "           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    word_num = 100\n",
    "    data_num = 25000\n",
    "    \n",
    "    # Keras에 내장되어 있는 imdb 데이터 세트를 불러오고 전처리합니다.\n",
    "    \n",
    "    (train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words = word_num)\n",
    "    \n",
    "    train_data = sequences_shaping(train_data, dimension = word_num)\n",
    "    test_data = sequences_shaping(test_data, dimension = word_num)\n",
    "    \n",
    "    adagrad_model = None  # Adagrad를 사용할 모델입니다.\n",
    "    rmsprop_model = None  # RMSProp을 사용할 모델입니다.\n",
    "    adam_model = None     # Adam을 사용할 모델입니다.\n",
    "    \n",
    "    adagrad_opt = None\n",
    "    adagrad_model.compile(None)\n",
    "    \n",
    "    rmsprop_opt = None\n",
    "    rmsprop_model.compile(None)\n",
    "    \n",
    "    adam_opt = None\n",
    "    adam_model.compile(None)\n",
    "    \n",
    "    None\n",
    "    None\n",
    "    None\n",
    "    \n",
    "    adagrad_history = None\n",
    "    print('\\n')\n",
    "    rmsprop_history = None\n",
    "    print('\\n')\n",
    "    adam_history = None\n",
    "    \n",
    "    scores_adagrad = None\n",
    "    scores_rmsprop = None\n",
    "    scores_adam = None\n",
    "    \n",
    "    print('\\nscores_adagrad: ', scores_adagrad[-1])\n",
    "    print('scores_rmsprop: ', scores_rmsprop[-1])\n",
    "    print('scores_adam: ', scores_adam[-1])\n",
    "    \n",
    "    Visulaize([('Adagrad', adagrad_history),('RMSprop', rmsprop_history),('Adam', adam_history)])\n",
    "    \n",
    "    return adagrad_history, rmsprop_history, adam_history\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab6e8f",
   "metadata": {},
   "source": [
    "# 실습4\n",
    "- visual.py 파일을 만들어서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "# tensorflow와 tf.keras를 임포트합니다\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def Visulaize(histories, key='sparse_categorical_crossentropy'):\n",
    "\n",
    "    for name, history in histories:\n",
    "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                   '--', label=name.title()+' Val')\n",
    "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "             label=name.title()+' Train')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(key.replace('_',' ').title())\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlim([0,max(history.epoch)])\n",
    "    \n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from visual import *\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "'''\n",
    "1. 다양한 최적화 알고리즘들을 적용할 하나의 모델을 자유롭게 생성합니다.\n",
    "'''\n",
    "\n",
    "def OUR_model():\n",
    "    \n",
    "    model = tf.keras.Sequential([None])\n",
    "    \n",
    "    return model\n",
    "\n",
    "'''\n",
    "2. 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. OUR_model 함수를 이용해 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 모델의 손실 함수, 최적화 알고리즘, 평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 모델을 각각 학습시킵니다. 검증용 데이터도 설정해주세요.\n",
    "           모델의 'epochs'는 20, 'batch_size'는 500으로 설정합니다.\n",
    "   \n",
    "   Step04. 학습된 모델을 테스트하고 sparse categorical crossentropy\n",
    "           값을 출력합니다. 모델의 성능을 확인해보고, 목표값을 달성해보세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Fashion mnist data 를 load 합니다.\n",
    "    train_data = np.loadtxt('./data/train_images.csv', delimiter =',', dtype = np.float32)\n",
    "    train_labels = np.loadtxt('./data/train_labels.csv', delimiter =',', dtype = np.float32)\n",
    "    test_data = np.loadtxt('./data/test_images.csv', delimiter =',', dtype = np.float32)\n",
    "    test_labels = np.loadtxt('./data/test_labels.csv', delimiter =',', dtype = np.float32)\n",
    "    \n",
    "    train_data = train_data / 255.0\n",
    "    test_data = test_data / 255.0\n",
    "    \n",
    "    our_model = None\n",
    "    \n",
    "    opt = None\n",
    "    our_model.compile(None)\n",
    "    history = None\n",
    "    \n",
    "    scores = None\n",
    "    \n",
    "    print('\\nscores: ', scores[-1])\n",
    "    \n",
    "    Visulaize([('our_model', history)])\n",
    "    \n",
    "    return history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d91d99",
   "metadata": {},
   "source": [
    "# 실습5\n",
    "-        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "- tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "        tf.keras.layers.Dense(32, activation='sigmoid'),\n",
    "- make_model_relu()\n",
    "- make_model_sig()\n",
    "- loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']\n",
    "- loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']\n",
    "- model_relu.summary()\n",
    "- model_sig.summary()\n",
    "- model_relu.fit(x_train, y_train, epochs=5, batch_size=500, verbose=0)\n",
    "- model_sig.fit(x_train, y_train, epochs=5, batch_size=500, verbose=0)\n",
    "- model_relu.evaluate(x_test, y_test, verbose=0)\n",
    "- model_sig.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de92cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "'''\n",
    "1. 활성화 함수는 출력층만 그대로 두고 \n",
    "   나머지 히든층들은 `relu`로 설정하세요.\n",
    "'''\n",
    "\n",
    "def make_model_relu():\n",
    "    \n",
    "    model_relu = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        None,\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model_relu\n",
    "    \n",
    "'''\n",
    "2. 활성화 함수는 출력층만 그대로 두고\n",
    "   나머지 히든층들은 `sigmoid`로 설정하세요.\n",
    "'''\n",
    "    \n",
    "def make_model_sig():\n",
    "    \n",
    "    model_sig = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        None,\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model_sig\n",
    "\n",
    "'''\n",
    "3. 두 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. model_relu와 model_sig 불러옵니다.\n",
    "   \n",
    "   Step02. 두 모델의 최적화 방법과 손실 함수를 \n",
    "           똑같이 설정합니다.\n",
    "   \n",
    "   Step03. 두 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "           우리가 만든 모델이 얼마나 깊은지 확인해보세요.\n",
    "   \n",
    "   Step04. 두 모델을 학습시킵니다. \n",
    "           'epochs'는 5로 설정합니다.\n",
    "           검증용 데이터는 설정하지 않습니다. \n",
    "           'verbose'는 0으로 설정합니다.\n",
    "   \n",
    "   Step05. 두 모델을 테스트하고 점수를 출력합니다. \n",
    "           둘 중 어느 모델의 성능이 더 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # MNIST 데이터를 불러오고 전처리합니다.\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    \n",
    "    model_relu = None  # 히든층들의 활성화 함수로 relu를 쓰는 모델입니다.\n",
    "    model_sig = None   # 히든층들의 활성화 함수로 sigmoid를 쓰는 모델입니다.\n",
    "    \n",
    "    model_relu.compile(None)\n",
    "    model_sig.compile(None)\n",
    "    \n",
    "    None\n",
    "    None\n",
    "    \n",
    "    model_relu_history = None\n",
    "    print('\\n')\n",
    "    model_sig_history = None\n",
    "    \n",
    "    scores_relu = None\n",
    "    scores_sig = None\n",
    "    \n",
    "    print('\\naccuracy_relu: ', scores_relu[-1])\n",
    "    print('accuracy_sig: ', scores_sig[-1])\n",
    "    \n",
    "    return model_relu_history, model_sig_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de841cfb",
   "metadata": {},
   "source": [
    "# 실습6\n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9550ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    \n",
    "'''\n",
    "1. 활성화 함수는 출력층만 그대로 두고 \n",
    "   나머지 히든층들은 'sigmoid'로 설정하세요.\n",
    "'''\n",
    "    \n",
    "def make_model_sig():\n",
    "    \n",
    "    model_sig = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        None,\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model_sig\n",
    "    \n",
    "'''\n",
    "2. 활성화 함수는 출력층만 그대로 두고 \n",
    "   나머지 히든층들은 'relu'로 설정하세요.\n",
    "'''\n",
    "\n",
    "def make_model_relu():\n",
    "    \n",
    "    model_relu = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        None,\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model_relu\n",
    "\n",
    "    \n",
    "'''\n",
    "3. 활성화 함수는 출력층만 그대로 두고 \n",
    "   나머지 히든층들은 'tanh'로 설정하세요.\n",
    "'''\n",
    "    \n",
    "def make_model_tanh():\n",
    "    \n",
    "    model_tanh = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        None,\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model_tanh\n",
    "\n",
    "'''\n",
    "4. 세 개의 모델을 불러온 후 학습시키고 테스트 데이터에 대해 평가합니다.\n",
    "\n",
    "   Step01. make_model_sig, make_model_relu, make_model_tanh 함수를 이용해 세 모델을 불러옵니다.\n",
    "   \n",
    "   Step02. 세 모델의 손실 함수, 최적화 알고리즘, \n",
    "          평가 방법을 설정합니다.\n",
    "   \n",
    "   Step03. 세 모델의 구조를 확인하는 코드를 작성합니다.\n",
    "           우리가 만든 모델이 얼마나 깊은지 확인해보세요.\n",
    "   \n",
    "   Step04. 세 모델을 학습시킵니다. \n",
    "           'epochs'는 5로 설정합니다.\n",
    "           검증용 데이터는 설정하지 않습니다.\n",
    "   \n",
    "   Step05. 세 모델을 테스트하고 accuracy 값을 출력합니다. \n",
    "           셋 중 어느 모델의 성능이 가장 좋은지 확인해보세요.\n",
    "'''\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # MNIST 데이터를 불러오고 전처리합니다.\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    \n",
    "    model_sig = None   # 히든층들의 활성화 함수로 sigmoid를 쓰는 모델입니다.\n",
    "    model_relu = None  # 히든층들의 활성화 함수로 relu를 쓰는 모델입니다.\n",
    "    model_tanh = None  # 히든층들의 활성화 함수로 tanh를 쓰는 모델입니다.\n",
    "    \n",
    "    model_sig.compile(None)\n",
    "    model_relu.compile(None)\n",
    "    model_tanh.compile(None)\n",
    "    \n",
    "    None\n",
    "    None\n",
    "    None\n",
    "    \n",
    "    model_sig_history = None\n",
    "    print('\\n')\n",
    "    model_relu_history = None\n",
    "    print('\\n')\n",
    "    model_tanh_history = None\n",
    "    \n",
    "    scores_sig = None\n",
    "    scores_relu = None\n",
    "    scores_tanh = None\n",
    "    \n",
    "    print('\\naccuracy_sig: ', scores_sig[-1])\n",
    "    print('accuracy_relu: ', scores_relu[-1])\n",
    "    print('accuracy_tanh: ', scores_tanh[-1])\n",
    "    \n",
    "    return model_sig_history, model_relu_history, model_tanh_history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a15ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8257c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf25dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cc8813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa8196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198dd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67841eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa1e4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd1ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eccf3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766297a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a0f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b867bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc19a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d557103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033471bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f3088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd043bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421132b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db12a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252710c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f8828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764499f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbd683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bb2dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ade8b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69908da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d046a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93795355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6797e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16a8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3928a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ec664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372cf2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80246c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796fc30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6cd1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118476d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30e99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae467ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fe250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959c9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2c6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
