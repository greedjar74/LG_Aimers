{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fbe20a",
   "metadata": {},
   "source": [
    "# Vanilla RNN으로 IMDb 데이터 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229a575",
   "metadata": {},
   "source": [
    "지시사항\n",
    "Tensorflow는 유명한 데이터셋의 일부를 라이브러리에서 바로 활용할 수 있는 API를 제공하고 있습니다. 이 데이터셋들은tensorflow.keras.datasets이라는 모듈에서 찾아볼 수 있습니다. IMDb 데이터셋 또한 이 모듈에서 제공하고 있기 때문에 여기서는 이를 활용하여 데이터셋을 불러오도록 하겠습니다.\n",
    "\n",
    "앞서 언급했듯 이 데이터셋은 긍정과 부정 두가지의 클래스를 가지고 있으니 이진 분류(Binary Classification)을 할 수 있는 모델을 구성하도록 하겠습니다.\n",
    "\n",
    "지시사항에 따라 코드를 완성하세요.\n",
    "\n",
    "SimpleRNN 기반 모델을 완성하는 함수 build_rnn_model을 완성하세요. 모델의 Layer 구성은 다음과 같습니다.\n",
    "layers.Embedding\n",
    "전체 단어 개수: num_words\n",
    "벡터 길이: embedding_len\n",
    "layers.SimpleRNN\n",
    "hidden state 크기: 16\n",
    "layers.Dense\n",
    "노드 개수: 1개\n",
    "활성화 함수: Sigmoid\n",
    "\n",
    "main 함수에서 모델 학습을 위한 Optimizer, 손실 함수, 평가 지표(Metrics)을 설정하세요.\n",
    "Optimizer: Adam\n",
    "Learning rate: 0.001\n",
    "손실 함수: binary_crossentropy\n",
    "평가 지표: accuracy\n",
    "\n",
    "모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "epochs=epochs\n",
    "batch_size=100\n",
    "validation_split=0.2\n",
    "shuffle=True\n",
    "verbose=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae50c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(num_words, max_len):\n",
    "    # imdb 데이터셋을 불러옵니다. 데이터셋에서 단어는 num_words 개를 가져옵니다.\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "    # 단어 개수가 다른 문장들을 Padding을 추가하여\n",
    "    # 단어가 가장 많은 문장의 단어 개수로 통일합니다.\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_rnn_model(num_words, embedding_len):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # TODO: [지시사항 1번] 지시사항에 따라 모델을 완성하세요.\n",
    "    model.add(None)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main(model=None, epochs=5):\n",
    "    # IMDb 데이터셋에서 가져올 단어의 개수\n",
    "    num_words = 6000\n",
    "    \n",
    "    # 각 문장이 가질 수 있는 최대 단어 개수\n",
    "    max_len = 130\n",
    "    \n",
    "    # 임베딩 된 벡터의 길이\n",
    "    embedding_len = 100\n",
    "    \n",
    "    # IMDb 데이터셋을 불러옵니다.\n",
    "    X_train, X_test, y_train, y_test = load_data(num_words, max_len)\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_rnn_model(num_words, embedding_len)\n",
    "    \n",
    "    # TODO: [지시사항 2번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = None\n",
    "    None\n",
    "    \n",
    "    # TODO: [지시사항 3번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = None\n",
    "    \n",
    "    # 모델을 테스트 데이터셋으로 테스트합니다.\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print(\"테스트 Loss: {:.5f}, 테스트 정확도: {:.3f}%\".format(test_loss, test_acc * 100))\n",
    "    \n",
    "    return optimizer, hist\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94dd33e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 - 3s - 17ms/step - accuracy: 0.7051 - loss: 0.5771 - val_accuracy: 0.7630 - val_loss: 0.5070\n",
      "Epoch 2/5\n",
      "200/200 - 3s - 14ms/step - accuracy: 0.8378 - loss: 0.3868 - val_accuracy: 0.7324 - val_loss: 0.6022\n",
      "Epoch 3/5\n",
      "200/200 - 3s - 14ms/step - accuracy: 0.8860 - loss: 0.2953 - val_accuracy: 0.8088 - val_loss: 0.4296\n",
      "Epoch 4/5\n",
      "200/200 - 3s - 15ms/step - accuracy: 0.9431 - loss: 0.1717 - val_accuracy: 0.8240 - val_loss: 0.4398\n",
      "Epoch 5/5\n",
      "200/200 - 3s - 14ms/step - accuracy: 0.9725 - loss: 0.0986 - val_accuracy: 0.8210 - val_loss: 0.5062\n",
      "\n",
      "테스트 Loss: 0.51232, 테스트 정확도: 82.208%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(num_words, max_len):\n",
    "    # imdb 데이터셋을 불러옵니다. 데이터셋에서 단어는 num_words 개를 가져옵니다.\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "    # 단어 개수가 다른 문장들을 Padding을 추가하여 -> sequence 데이터는 길이가 다르다.\n",
    "    # 단어가 가장 많은 문장의 단어 개수로 통일합니다.\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def build_rnn_model(num_words, embedding_len):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # TODO: [지시사항 1번] 지시사항에 따라 모델을 완성하세요.\n",
    "    model.add(tf.keras.layers.Embedding(num_words, embedding_len))\n",
    "    model.add(tf.keras.layers.SimpleRNN(16))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # 긍정일 확률을 구한다.\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main(model=None, epochs=5):\n",
    "    # IMDb 데이터셋에서 가져올 단어의 개수\n",
    "    num_words = 6000\n",
    "    \n",
    "    # 각 문장이 가질 수 있는 최대 단어 개수\n",
    "    max_len = 130\n",
    "    \n",
    "    # 임베딩 된 벡터의 길이\n",
    "    embedding_len = 100\n",
    "    \n",
    "    # IMDb 데이터셋을 불러옵니다.\n",
    "    X_train, X_test, y_train, y_test = load_data(num_words, max_len)\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_rnn_model(num_words, embedding_len)\n",
    "    \n",
    "    # TODO: [지시사항 2번] 모델 학습을 위한 optimizer와 loss 함수를 설정하세요.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # TODO: [지시사항 3번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=100, validation_split=0.2, shuffle=True, verbose=2)\n",
    "    \n",
    "    # 모델을 테스트 데이터셋으로 테스트합니다.\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print(\"테스트 Loss: {:.5f}, 테스트 정확도: {:.3f}%\".format(test_loss, test_acc * 100))\n",
    "    \n",
    "    return optimizer, hist\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b6985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a388388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc055d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
