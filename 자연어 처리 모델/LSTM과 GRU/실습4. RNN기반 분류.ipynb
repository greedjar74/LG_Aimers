{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0c190d",
   "metadata": {},
   "source": [
    "# RNN 기반 모델을 통한 분류 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070d1a4",
   "metadata": {},
   "source": [
    "지시사항\n",
    "SimpleRNN 기반 모델을 만드는 함수 build_rnn_model을 완성하세요. 각 Layer 구성은 아래와 같습니다.\n",
    "layers.Embedding: max_features에서 embedding_size로 각 문장을 구성하는 벡터의 크기를 줄이는 임베딩 레이어를 추가하세요.\n",
    "layers.SimpleRNN: hidden state 크기를 20으로 설정하세요.\n",
    "layers.Dense\n",
    "노드 개수: 5개\n",
    "활성화 함수: Softmax\n",
    "\n",
    "LSTM 기반 모델을 만드는 함수 build_lstm_model을 완성하세요. 각 Layer 구성은 아래와 같습니다.\n",
    "layers.Embedding: max_features에서 embedding_size로 각 문장을 구성하는 벡터의 크기를 줄이는 임베딩 레이어를 추가하세요.\n",
    "layers.LSTM: hidden state 크기를 20으로 설정하세요.\n",
    "layers.Dense\n",
    "노드 개수: 5개\n",
    "활성화 함수: Softmax\n",
    "\n",
    "GRU 기반 모델을 만드는 함수 build_gru_model을 완성하세요. 각 Layer 구성은 아래와 같습니다.\n",
    "layers.Embedding: max_features에서 embedding_size로 각 문장을 구성하는 벡터의 크기를 줄이는 임베딩 레이어를 추가하세요.\n",
    "layers.GRU: hidden state 크기를 20으로 설정하세요.\n",
    "layers.Dense\n",
    "노드 개수: 5개\n",
    "활성화 함수: Softmax\n",
    "\n",
    "run_model 함수 내에서 모델 학습을 위한 optimizer, loss 함수, 평가 지표(metrics)를 설정하세요.\n",
    "Optimizer: Adam\n",
    "Learning Rate: 0.001\n",
    "Loss 함수: sparse_categorical_crossentropy\n",
    "평가 지표: Accuracy\n",
    "\n",
    "run_model 함수 내에서 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "batch_size=256\n",
    "epochs=epochs\n",
    "shuffle=True\n",
    "verbose=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b920e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(max_len):\n",
    "    data = pd.read_csv(\"./review_score.csv\")\n",
    "    # 리뷰 문장을 입력 데이터로, 해당 리뷰의 평점을 라벨 데이터로 설정합니다.\n",
    "    X = data['Review']\n",
    "    y = data['Score']\n",
    "    y = y - 1 # 값을 1~5에서 0~4로 변경\n",
    "\n",
    "    # 문장 내 각 단어를 숫자로 변환하는 Tokenizer를 적용합니다.\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    # 전체 단어 중에서 가장 큰 숫자로 mapping된 단어의 숫자를 가져옵니다.\n",
    "    # 즉, max_features는 전체 데이터셋에 등장하는 겹치지 않는 단어의 개수 + 1과 동일합니다.\n",
    "    max_features = max([max(_in) for _in in X]) + 1\n",
    "\n",
    "    # 불러온 데이터셋을 학습 데이터 80%, 테스트 데이터 20%로 분리합니다.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # 모든 문장들을 가장 긴 문장의 단어 개수가 되게 padding을 추가합니다.\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, max_features\n",
    "\n",
    "def build_rnn_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] Simple RNN 기반의 모델을 완성하세요.\n",
    "    model.add(None)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] LSTM 기반의 모델을 완성하세요.\n",
    "    model.add(None)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_gru_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 3번] GRU 기반의 모델을 완성하세요.\n",
    "    model.add(None)\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=10):\n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 optimizer, loss 함수, 평가 지표를 설정하세요.\n",
    "    optimizer = None\n",
    "    None\n",
    "\n",
    "    # TODO: [지시사항 5번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = None\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    return test_loss, test_acc, optimizer, hist\n",
    "\n",
    "def main():\n",
    "    tf.random.set_seed(2022)\n",
    "    max_len = 150\n",
    "    embedding_size = 128\n",
    "\n",
    "    X_train, X_test, y_train, y_test, max_features = load_data(max_len)\n",
    "    rnn_model = build_rnn_model(max_features, embedding_size)\n",
    "    lstm_model = build_lstm_model(max_features, embedding_size)\n",
    "    gru_model = build_gru_model(max_features, embedding_size)\n",
    "\n",
    "    rnn_test_loss, rnn_test_acc, _, _ = run_model(rnn_model, X_train, X_test, y_train, y_test)\n",
    "    lstm_test_loss, lstm_test_acc, _, _ = run_model(lstm_model, X_train, X_test, y_train, y_test)\n",
    "    gru_test_loss, gru_test_acc, _, _ = run_model(gru_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 20, \"모델 별 Test Loss와 정확도\", \"=\" * 20)\n",
    "    print(\"[RNN ] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(rnn_test_loss, rnn_test_acc * 100))\n",
    "    print(\"[LSTM] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(lstm_test_loss, lstm_test_acc * 100))\n",
    "    print(\"[GRU ] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(gru_test_loss, gru_test_acc * 100))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(max_len):\n",
    "    data = pd.read_csv(\"./review_score.csv\")\n",
    "    # 리뷰 문장을 입력 데이터로, 해당 리뷰의 평점을 라벨 데이터로 설정합니다.\n",
    "    X = data['Review']\n",
    "    y = data['Score']\n",
    "    y = y - 1 # 값을 1~5에서 0~4로 변경\n",
    "\n",
    "    # 문장 내 각 단어를 숫자로 변환하는 Tokenizer를 적용합니다.\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    # 전체 단어 중에서 가장 큰 숫자로 mapping된 단어의 숫자를 가져옵니다.\n",
    "    # 즉, max_features는 전체 데이터셋에 등장하는 겹치지 않는 단어의 개수 + 1과 동일합니다.\n",
    "    max_features = max([max(_in) for _in in X]) + 1\n",
    "\n",
    "    # 불러온 데이터셋을 학습 데이터 80%, 테스트 데이터 20%로 분리합니다.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # 모든 문장들을 가장 긴 문장의 단어 개수가 되게 padding을 추가합니다.\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, max_features\n",
    "\n",
    "def build_rnn_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 1번] Simple RNN 기반의 모델을 완성하세요.\n",
    "    model.add(tf.keras.layers.Embedding(max_features, embedding_size))\n",
    "    model.add(tf.keras.layers.SimpleRNN(20))\n",
    "    model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 2번] LSTM 기반의 모델을 완성하세요.\n",
    "    model.add(tf.keras.layers.Embedding(max_features, embedding_size))\n",
    "    model.add(tf.keras.layers.LSTM(20))\n",
    "    model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_gru_model(max_features, embedding_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    # TODO: [지시사항 3번] GRU 기반의 모델을 완성하세요.\n",
    "    model.add(tf.keras.layers.Embedding(max_features, embedding_size))\n",
    "    model.add(tf.keras.layers.GRU(20))\n",
    "    model.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_model(model, X_train, X_test, y_train, y_test, epochs=10):\n",
    "    # TODO: [지시사항 4번] 모델 학습을 위한 optimizer, loss 함수, 평가 지표를 설정하세요.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # TODO: [지시사항 5번] 모델 학습을 위한 hyperparameter를 설정하세요.\n",
    "    hist = model.fit(X_train, y_train, batch_size=256, epochs=epochs, shuffle=True, verbose=2)\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    return test_loss, test_acc, optimizer, hist\n",
    "\n",
    "def main():\n",
    "    tf.random.set_seed(2022)\n",
    "    max_len = 150\n",
    "    embedding_size = 128\n",
    "\n",
    "    X_train, X_test, y_train, y_test, max_features = load_data(max_len)\n",
    "    rnn_model = build_rnn_model(max_features, embedding_size)\n",
    "    lstm_model = build_lstm_model(max_features, embedding_size)\n",
    "    gru_model = build_gru_model(max_features, embedding_size)\n",
    "\n",
    "    rnn_test_loss, rnn_test_acc, _, _ = run_model(rnn_model, X_train, X_test, y_train, y_test)\n",
    "    lstm_test_loss, lstm_test_acc, _, _ = run_model(lstm_model, X_train, X_test, y_train, y_test)\n",
    "    gru_test_loss, gru_test_acc, _, _ = run_model(gru_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 20, \"모델 별 Test Loss와 정확도\", \"=\" * 20)\n",
    "    print(\"[RNN ] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(rnn_test_loss, rnn_test_acc * 100))\n",
    "    print(\"[LSTM] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(lstm_test_loss, lstm_test_acc * 100))\n",
    "    print(\"[GRU ] 테스트 Loss: {:.5f}, 테스트 Accuracy: {:.3f}%\".format(gru_test_loss, gru_test_acc * 100))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
